<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jinyu-m.github.io</id>
    <title>诗酒趁年华 年少万兜鍪</title>
    <updated>2020-11-05T01:43:22.692Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jinyu-m.github.io"/>
    <link rel="self" href="https://jinyu-m.github.io/atom.xml"/>
    <subtitle>Jinyu Miao
尘世恰好，有诗有酒刚好吐槽</subtitle>
    <logo>https://jinyu-m.github.io/images/avatar.png</logo>
    <icon>https://jinyu-m.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 诗酒趁年华 年少万兜鍪</rights>
    <entry>
        <title type="html"><![CDATA[Deep Local Features: SuperPoint]]></title>
        <id>https://jinyu-m.github.io/post/superpoint/</id>
        <link href="https://jinyu-m.github.io/post/superpoint/">
        </link>
        <updated>2020-11-04T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="superpoint-magic-leap-pdf">SuperPoint (Magic Leap) <a href="https://arxiv.org/abs/1712.07629.pdf">pdf</a></h2>
<h3 id="abstract"><em>Abstract</em></h3>
<p>作者在MagicPoint的基础上增加了提取descriptor的部分，提出了homographic adaptation的数据增强方法，用来训练detector的repeatability，并且使得训练数据可以从虚拟的仿真数据拓展到MS-COCO等真实数据。</p>
<h3 id="introduction"><em>Introduction</em></h3>
<p>作者认为deep feature训练的关键在于带有标签的数据，但是人工标注的数据中，关键点往往是ill-defined，所以作者提出自监督方法训练，用网络本身去标注一批伪真值关键点，在此基础上进行训练，这样做，关键点更丰富，标注成本也更低。</p>
<p>![train](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_1.png)</p>
<p>第一步，其实就是之前提到的训练MagicPoint，先构建了一个仿真数据集Synthetic Shapes，训练了MagicPoint(b)。虽然之前的论文提到MagicPoint在应对各种干扰时重复率和准确率都很高，但是它丢失了一些潜在的关键点，为了解决这个问题，作者用Homographic Adaption去增强了MagicPoint标注的真实图像，得到了一个更符合预期的真实数据(b)，并用此去训练一个新的网络SuperPoint(c).</p>
<h3 id="architecture"><em>Architecture</em></h3>
<p>![architecture](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_2.png)</p>
<p>SuperPoint由一个共享参数的encoder和两个task-specific decoders构成，采用vgg结构，图像(H x W)通过encoder，得到一个1/8大小(Hc x Wc)的feature map。</p>
<p>在<strong>interest point decoder</strong>中，网络依旧延续MagicPoint的做法，采用非参数的上采样过程，feature map通过head降维到65 x Hc x Wc维，分别代表原图中与之对应的8x8区域内每个点是关键点的概率以及一个dustbin通道，dustbin用以表示该8x8区域内无关键点。</p>
<p>在<strong>descriptor decoder</strong>中，网络先提取了Hc x Wc的semi-dense descriptor，每个descriptor代表与之对应的8x8区域内关键点的256-d descriptor（根据interest point detector，每8x8区域内只可能存在1/0个关键点），然后采用bi-cubic插值恢复到原分辨率。</p>
<h3 id="loss"><em>Loss</em></h3>
<p>训练interest point detector依旧采用对每个cell计算cross-entropy loss的方法：</p>
<p>![loss](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_3.png)</p>
<p>为了使这部分Lp降低，需要让Y中为1的位置（即该点为关键点）在X中有较大的值，即增大该点为关键点的概率。</p>
<p>训练descriptor extractor时，需要先找到匹配的点，然后用匹配点的descriptor来计算loss，所以先找判断图1(h,w)和图2(h',w')是否是一组匹配点：</p>
<p>![loss](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_4.png)</p>
<p>p是cell的中心位置，H是真值homography，所以上式就是判断两个cell在原图中中心是否是符合真值homography的，如果是，则这两个位置时一对匹配点，可以计算它们descriptor的距离了：</p>
<p>![loss](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_5.png)</p>
<p>上式中当图1和图2中两个点是匹配点时，s=1，则两个点的descriptor之间的cosine距离应该越大越好；而当两个点不匹配时，s=0，两个点的descriptor间的cosine距离越小越好。此处作者采用了hinge loss。</p>
<p>综上，SuperPoint训练使用的loss：</p>
<p>![loss](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_6.png)</p>
<p>分别计算图1和图2与真值图像的interest point loss，再计算图1与图2间的descriptor loss。</p>
<h3 id="training"><em>Training</em></h3>
<p>作者先训练了SuperPoint中提取关键点的detector pathway，其实就是MagicPoint，发现在虚拟数据集上MP表现很好，在真实数据中，当场景中有大量角点时，效果很好，但是在自然场景中，MP效果不如传统特征，所以作者提出<strong>用自监督方法在真实场景中训练网络</strong>，即Homographic Adaptation。</p>
<h3 id="homographic-adaptation"><em>Homographic Adaptation</em></h3>
<p>![ha](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\superpoint_7.png)</p>
<p>可通过iterative homographic adaptation来提升效果。100次random homography效果较好。</p>
<h3 id="一些见解"><em>一些见解</em></h3>
<p>作为MagicPoint的升级版，SuperPoint我感觉其实是作者也发现了MagicPoint只是单纯的去学习检测人工标注的那些点，很局限，所以提出了homographic adaptation去boostrap自己。并且加入了descriptor decoder，让整个系统更完整了。但是对比后面的特征，可能没有考虑到特征的repeatable这种特性如何在训练中应用。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Local Features: MagicPoint]]></title>
        <id>https://jinyu-m.github.io/post/deep-local-features-magicpoint/</id>
        <link href="https://jinyu-m.github.io/post/deep-local-features-magicpoint/">
        </link>
        <updated>2020-11-04T01:30:16.000Z</updated>
        <content type="html"><![CDATA[<h2 id="magicpoint-magic-leap-pdf">MagicPoint (Magic Leap) <a href="https://arxiv.org/abs/1707.07410.pdf">pdf</a></h2>
<h3 id="abstract"><em>Abstract</em></h3>
<p>这篇论文提出了一种基于两个DCN的point tracking system。第一个DCN就是MagicPoint，提取图像的显著二维坐标点**（就只是一个detector！）<strong>；第二个DCN是MagicWrap，输入利用MagicPoint得到的一对图像中的二维坐标点信息，直接预测homography，</strong>（不需要descriptor！）**</p>
<h3 id="introduction"><em>Introduction</em></h3>
<p>作者先抛出了一个问题</p>
<blockquote>
<p>what would it take to build an ImageNet for SLAM?</p>
<p>What would it take to build DeepSLAM?</p>
</blockquote>
<p>由于SLAM领域的真实数据往往无法获得很好的标注，而仿真数据无法囊括现实中的所有变化，所以可能引起domain adaptation issues和过拟合。所以用data-driven的深度学习方法去解决SLAM问题尚未解决。</p>
<p>作者提到了两个点，首先利用预测图像的DCN去估计ego-motion是可能得，作者没有使用直接用图像估计6DoF位姿的监督方法，而是更关注geometric consistency；其次作者发现对于SLAM系统来说，预测和对齐关键点已经足够去解算pose，那么就不用去预测整幅图像了，直接估计homography足以满足需求。</p>
<h3 id="method"><em>Method</em></h3>
<h4 id="overview">overview</h4>
<p>![overview](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\magicpoint_1.png)</p>
<h4 id="magicpoint">MagicPoint</h4>
<p>作者设计MagicPoint的motivation就是认为hand-crafted detector需要过多的经验和技巧，往往无法cover所有的干扰，所以就直接用DCN去估计pixel-level的显著性，提取图像关键点。</p>
<p>![magicpoint](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\magicpoint_2.png)</p>
<p>结构类似于VGG。输入一个图像，得到一个同等分辨率的point response image，输出的每个pixel的值代表原图中这个位置是角点的概率。但是直接用encoder下采样-decoder上采样的结构恢复分辨率很耗算力，所以作者用网络得到了1/8大小的feature map，维度是65维（65个通道），这65个通道对应原图中不重叠的8x8的区域即一个dustbin通道（用于表示该8x8区域内无关键点），最后reshape到原本分辨率，这样decoder就没有参数了。</p>
<p>训练时使用OpenCV作了一批虚拟的几何体，几何体的角点可以直接得到，然后加入噪声、光照变化等进行数据增强。训练时对feature map上每个cell计算cross-entropy loss。</p>
<h4 id="magicwarp">MagicWarp</h4>
<p>MagicWarp输入一对图像的关键点，然后估计homography。比如两幅120x160的图像输入MagicPoint，分别得到65x15x20的feature map。输入MagicWarp后，先从channel维度上进行concatenation，得到130x15x20的feature map，然后经过一个VGG型的encoder，再通过全连接层降维，得到一个9-d的向量，恢复成3x3的homography矩阵。</p>
<p>![magicwarp](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\magicpoint_3.png)</p>
<p>训练时，用虚拟数据采集虚拟三维几何体的图像，来获得训练数据，计算loss时，用估计的homography将图1的point投影到图2，然后计算投影误差，用L2-distance作为loss。</p>
<p>![loss](F:\MyBUAA\硕SY1903703\笔记\deep local features\figs\magicpoint_4.png)</p>
<h3 id="一些看法"><em>一些看法</em></h3>
<p>MagicPoint算比较早的learnt detector了吧，所以感觉其实就是在学习如何检测corner，完全取决于annotated data中keypoint的标注位置。比较有意思的点在于非参数上采样过程，depth-to-space的处理方法很有借鉴意义。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于]]></title>
        <id>https://jinyu-m.github.io/post/about/</id>
        <link href="https://jinyu-m.github.io/post/about/">
        </link>
        <updated>2019-01-25T11:09:48.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>欢迎来到我的小站呀，很高兴遇见你！🤝</p>
</blockquote>
<h2 id="关于本站">🏠 关于本站</h2>
<h2 id="博主是谁">👨‍💻 博主是谁</h2>
<h2 id="兴趣爱好">⛹ 兴趣爱好</h2>
<h2 id="联系我呀">📬 联系我呀</h2>
]]></content>
    </entry>
</feed>