<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Local Features | 年少万兜鍪</title>
<link rel="shortcut icon" href="https://jinyu-m.github.io/favicon.ico?v=1606141741240">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://jinyu-m.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Deep Local Features | 年少万兜鍪 - Atom Feed" href="https://jinyu-m.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="最近要做一个关于deep local feature for localization的工作，所以读了一些learnt feature的论文，记录一下，作为参考。也希望自己的工作得到评委认可呀😎

MagicPoint (Magic Le..." />
    <meta name="keywords" content="Deep learning" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://jinyu-m.github.io">
  <img class="avatar" src="https://jinyu-m.github.io/images/avatar.png?v=1606141741240" alt="">
  </a>
  <h1 class="site-title">
    年少万兜鍪
  </h1>
  <p class="site-description">
    少年吔
尘世恰好，有诗有酒刚好吐槽
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Deep Local Features
            </h2>
            <div class="post-info">
              <span>
                2020-10-01
              </span>
              <span>
                28 min read
              </span>
              
                <a href="https://jinyu-m.github.io/tag/dJ8NdBIXq/" class="post-tag">
                  # Deep learning
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <p>最近要做一个关于deep local feature for localization的工作，所以读了一些learnt feature的论文，记录一下，作为参考。也希望自己的工作得到评委认可呀😎</p>
<!-- more -->
<h1 id="magicpoint-magic-leap-pdf">MagicPoint (Magic Leap) <a href="https://arxiv.org/abs/1707.07410.pdf">pdf</a></h1>
<h2 id="abstract"><em>Abstract</em></h2>
<p>论文提出了一种基于两个DCN的point tracking system。第一个DCN就是MagicPoint，提取图像的显著二维坐标点（只有detector）；第二个DCN是MagicWrap，输入利用MagicPoint得到的一对图像中的二维坐标点信息，直接预测homography（不需要descriptor信息）。</p>
<h2 id="introduction"><em>Introduction</em></h2>
<p>作者先抛出了一个问题</p>
<blockquote>
<p>what would it take to build an ImageNet for SLAM?<br>
What would it take to build DeepSLAM?</p>
</blockquote>
<p>由于SLAM领域的真实数据往往无法获得很好的标注，而仿真数据无法囊括现实中的所有变化，所以可能引起domain adaptation issues和过拟合。所以用data-driven的深度学习方法去解决SLAM问题尚未解决。</p>
<p>作者提到了两个点，首先利用预测图像的DCN去估计ego-motion是可能得，作者没有使用直接用图像估计6DoF位姿的监督方法，而是更关注geometric consistency；其次作者发现对于SLAM系统来说，预测和对齐关键点已经足够去解算pose，那么就不用去预测整幅图像了，直接估计homography足以满足需求。</p>
<h2 id="method"><em>Method</em></h2>
<h3 id="overview">overview</h3>
<figure data-type="image" tabindex="1"><img src="https://jinyu-m.github.io/post-images/1604541682692.png" alt="" loading="lazy"></figure>
<h3 id="magicpoint">MagicPoint</h3>
<p>作者设计MagicPoint的motivation就是认为hand-crafted detector需要过多的经验和技巧，往往无法cover所有的干扰，所以就直接用DCN去估计pixel-level的显著性，提取图像关键点。</p>
<figure data-type="image" tabindex="2"><img src="https://jinyu-m.github.io/post-images/1604541717680.png" alt="" loading="lazy"></figure>
<p>结构类似于VGG。输入一个图像，得到一个同等分辨率的point response image，输出的每个pixel的值代表原图中这个位置是角点的概率。但是直接用encoder下采样-decoder上采样的结构恢复分辨率很耗算力，所以作者用网络得到了1/8大小的feature map，维度是65维（65个通道），这65个通道对应原图中不重叠的8x8的区域即一个dustbin通道（用于表示该8x8区域内无关键点），最后reshape到原本分辨率，这样decoder就没有参数了。</p>
<p>训练时使用OpenCV作了一批虚拟的几何体，几何体的角点可以直接得到，然后加入噪声、光照变化等进行数据增强。训练时对feature map上每个cell计算cross-entropy loss。</p>
<h3 id="magicwarp">MagicWarp</h3>
<p>MagicWarp输入一对图像的关键点，然后估计homography。比如两幅120x160的图像输入MagicPoint，分别得到65x15x20的feature map。输入MagicWarp后，先从channel维度上进行concatenation，得到130x15x20的feature map，然后经过一个VGG型的encoder，再通过全连接层降维，得到一个9-d的向量，恢复成3x3的homography矩阵。</p>
<figure data-type="image" tabindex="3"><img src="https://jinyu-m.github.io/post-images/1604541734864.png" alt="" loading="lazy"></figure>
<p>训练时，用虚拟数据采集虚拟三维几何体的图像，来获得训练数据，计算loss时，用估计的homography将图1的point投影到图2，然后计算投影误差，用L2-distance作为loss。</p>
<figure data-type="image" tabindex="4"><img src="https://jinyu-m.github.io/post-images/1604541741188.png" alt="" loading="lazy"></figure>
<h2 id="一些看法"><em>一些看法</em></h2>
<p>MagicPoint学习了如何检测corner，基本取决于annotated data中keypoint的标注位置。比较有意思的点在于非参数上采样过程，depth-to-space的处理方法很有借鉴意义。后续SuperPoint也采用了这样上采样的方式，在recover resolution的同时参数也比较少。</p>
<hr>
<h1 id="superpoint-magic-leap-pdf-official-code-society-code">SuperPoint (Magic Leap) <a href="https://arxiv.org/abs/1712.07629.pdf">pdf</a> <a href="https://github.com/magicleap/SuperPointPretrainedNetwork">official code</a> <a href="https://github.com/rpautrat/SuperPoint">society code</a></h1>
<h2 id="abstract-2"><em>Abstract</em></h2>
<p>作者在MagicPoint的基础上增加了提取descriptor的部分，提出了homographic adaptation的数据增强方法，用来训练detector的repeatability，并且使得训练数据可以从虚拟的仿真数据拓展到MS-COCO等真实数据。</p>
<h2 id="introduction-2"><em>Introduction</em></h2>
<p>作者认为deep feature训练的关键在于带有标签的数据，但是人工标注的数据中，关键点往往是ill-defined，所以作者提出自监督方法训练，用网络本身去标注一批伪真值关键点，在此基础上进行训练，这样做，关键点更丰富，标注成本也更低。</p>
<figure data-type="image" tabindex="5"><img src="https://jinyu-m.github.io/post-images/1604541876168.png" alt="" loading="lazy"></figure>
<p>第一步，其实就是之前提到的训练MagicPoint，先构建了一个仿真数据集Synthetic Shapes，训练了MagicPoint(b)。虽然之前的论文提到MagicPoint在应对各种干扰时重复率和准确率都很高，但是它丢失了一些潜在的关键点，为了解决这个问题，作者用Homographic Adaption去增强了MagicPoint标注的真实图像，得到了一个更符合预期的真实数据(b)，并用此去训练一个新的网络SuperPoint(c).</p>
<h2 id="architecture"><em>Architecture</em></h2>
<figure data-type="image" tabindex="6"><img src="https://jinyu-m.github.io/post-images/1604541894356.png" alt="" loading="lazy"></figure>
<p>SuperPoint由一个共享参数的encoder和两个task-specific decoders构成，采用vgg结构，图像(H x W)通过encoder，得到一个1/8大小(Hc x Wc)的feature map。</p>
<p>在<strong>interest point decoder</strong>中，网络依旧延续MagicPoint的做法，采用非参数的上采样过程，feature map通过head降维到65 x Hc x Wc维，分别代表原图中与之对应的8x8区域内每个点是关键点的概率以及一个dustbin通道，dustbin用以表示该8x8区域内无关键点。</p>
<p>在<strong>descriptor decoder</strong>中，网络先提取了Hc x Wc的semi-dense descriptor，每个descriptor代表与之对应的8x8区域内关键点的256-d descriptor（根据interest point detector，每8x8区域内只可能存在1/0个关键点），然后采用bi-cubic插值恢复到原分辨率。</p>
<h2 id="loss"><em>Loss</em></h2>
<p>训练interest point detector依旧采用对每个cell计算cross-entropy loss的方法：</p>
<figure data-type="image" tabindex="7"><img src="https://jinyu-m.github.io/post-images/1604542024924.png" alt="" loading="lazy"></figure>
<p>为了使这部分Lp降低，需要让Y中为1的位置（即该点为关键点）在X中有较大的值，即增大该点为关键点的概率。</p>
<p>训练descriptor extractor时，需要先找到匹配的点，然后用匹配点的descriptor来计算loss，所以先找判断图1(h,w)和图2(h',w')是否是一组匹配点：</p>
<figure data-type="image" tabindex="8"><img src="https://jinyu-m.github.io/post-images/1604542034183.png" alt="" loading="lazy"></figure>
<p>p是cell的中心位置，H是真值homography，所以上式就是判断两个cell在原图中中心是否是符合真值homography的，如果是，则这两个位置时一对匹配点，可以计算它们descriptor的距离了：</p>
<figure data-type="image" tabindex="9"><img src="https://jinyu-m.github.io/post-images/1604542039512.png" alt="" loading="lazy"></figure>
<p>上式中当图1和图2中两个点是匹配点时，s=1，则两个点的descriptor之间的cosine距离应该越大越好；而当两个点不匹配时，s=0，两个点的descriptor间的cosine距离越小越好。此处作者采用了hinge loss。</p>
<p>综上，SuperPoint训练使用的loss：</p>
<figure data-type="image" tabindex="10"><img src="https://jinyu-m.github.io/post-images/1604542045657.png" alt="" loading="lazy"></figure>
<p>分别计算图1和图2与真值图像的interest point loss，再计算图1与图2间的descriptor loss。</p>
<h2 id="training"><em>Training</em></h2>
<p>作者先训练了SuperPoint中提取关键点的detector pathway，其实就是MagicPoint，发现在虚拟数据集上MP表现很好，在真实数据中，当场景中有大量角点时，效果很好，但是在自然场景中，MP效果不如传统特征，所以作者提出<strong>用自监督方法在真实场景中训练网络</strong>，即Homographic Adaptation。</p>
<h2 id="homographic-adaptation"><em>Homographic Adaptation</em></h2>
<figure data-type="image" tabindex="11"><img src="https://jinyu-m.github.io/post-images/1604542062673.png" alt="" loading="lazy"></figure>
<p>可通过iterative homographic adaptation来提升效果。100次random homography效果较好。</p>
<h2 id="一些见解"><em>一些见解</em></h2>
<p>作为MagicPoint的升级版，SuperPoint我感觉其实是作者发现了MagicPoint只是单纯的去学习检测人工标注的那些点，比较局限，所以提出了homographic adaptation去进一步提升自己。并且加入了descriptor decoder，让整个系统更完整了。但是对比后续出现的特征，SuperPoint特征在训练时依旧采用监督的方法去训练detector，导致效果有待提升。在我的试验中，Bag of SuperPoint会提升loop closure的表现，但是用于SLAM系统（ORB-SLAM2）时，SuperPoint从一幅图像提取的特征过少，可能无法满足SLAM初始化的要求。</p>
<hr>
<h1 id="d2-net-pdf-code">D2-net <a href="https://arxiv.org/abs/1905.03561.pdf">pdf</a> <a href="https://github.com/mihaidusmanu/d2-net">code</a></h1>
<h2 id="abstract-3"><em>Abstract</em></h2>
<p>D2-net的主要在于提出了detect-and-describe的特征提取方法，不再是传统的detect-then-describe方法。作者认为从高层语义信息（CNN的高层conv输出的feature map）中提取的关键点位置要比从低层结构信息中提取的更稳定一些。detector和descriptor的参数实现了完全的共享。</p>
<h2 id="method-2"><em>Method</em></h2>
<p>D2的detector和descriptor是基于相同的feature map获取的。输入图像通过一个前向网络，得到C x H x W feature map，被视为得到了H x W 个C维的稠密局部特征。</p>
<figure data-type="image" tabindex="12"><img src="https://jinyu-m.github.io/post-images/1604543214624.png" alt="" loading="lazy"></figure>
<h2 id="hard-feature-detectiontest"><em>Hard feature detection(test)</em></h2>
<p>由于D2得到feature map有很多层，每一层都可以作为一个detector去检测局部最大值进而提取local feature，所以在提取特征时，D2约定（i，j）为一个特征点当且仅当在该点具有最大值的那个detector中，该点是一个局部最大值：</p>
<figure data-type="image" tabindex="13"><img src="https://jinyu-m.github.io/post-images/1604543229447.png" alt="" loading="lazy"></figure>
<p>直观地来讲，就是对于每个点，我们需要先找到该点对应C个detector中最显著的那个detector，然后验证在该detector中，当前点是否是局部最大的。</p>
<h2 id="soft-feature-detectiontraining"><em>Soft feature detection(training)</em></h2>
<p>但是上述detection方法是不可微的，所有无法用BP进行训练，为了实现end-to-end训练，作者soften了上述detection的方法。</p>
<p>首先soften筛选最显著detector的部分，计算当前detector的ratio-to-max用以表示其显著性</p>
<figure data-type="image" tabindex="14"><img src="https://jinyu-m.github.io/post-images/1604543241393.png" alt="" loading="lazy"></figure>
<p>其次soften计算局部最优值的部分，计算了9邻域内某点的soft local-max占比</p>
<figure data-type="image" tabindex="15"><img src="https://jinyu-m.github.io/post-images/1604543248965.png" alt="" loading="lazy"></figure>
<p>最后，综合两部分的分数，取最大值，并做image-level normalization得到一个用来表征像素是特征点的概率的score map。</p>
<h2 id="multiscale-detection"><em>Multiscale detection</em></h2>
<p>为了获取具有更强尺度不变性的特征，D2使用图像金字塔，在测试阶段，将图像分别缩放至0.5,1,2倍，输入D2，得到feature map，并将之前由低分辨率图像获得的feature map插值放大到当前分辨率，与当前feature map相加，获得更稳定的feature map，在此map上进行detection，之前提取的关键点也被上采样（最近邻）到当前尺度，纳入提取的特征中。</p>
<h2 id="data"><em>Data</em></h2>
<p>D2用megadepth数据集进行训练，megadepth提供了同一场景不同视角、光照、设备下的照片，以及深度信息，每个场景由COLMAP进行建图，由此获得2d-3d特征点的对应信息，利用sfm提供的信息，我们可以计算同一场景下两幅图像的overlap，在训练中，D2使用overlap&gt;0.5的图像对，并用depth信息进行验证，去除被遮挡的像素。</p>
<h2 id="loss-2"><em>Loss</em></h2>
<p>为了提升descriptor的区分度，需要让对应点的descriptor距离较小，非对应点的descriptor距离较大，所以D2使用了triplet margin loss。对于图1和图2中一对匹配点A和B，positive descriptor distance=</p>
<figure data-type="image" tabindex="16"><img src="https://jinyu-m.github.io/post-images/1604543266447.png" alt="" loading="lazy"></figure>
<p>在计算negative descriptor distance时，先挑选hardest negatives：</p>
<figure data-type="image" tabindex="17"><img src="https://jinyu-m.github.io/post-images/1604543274031.png" alt="" loading="lazy"></figure>
<p>直观理解，N1为图1中位于A的邻域之外，与B最相似的点。N2同理。</p>
<p>然后就可以分别计算A与N2，B与N1的descriptor距离，得到negative descriptor distance</p>
<figure data-type="image" tabindex="18"><img src="https://jinyu-m.github.io/post-images/1604543283391.png" alt="" loading="lazy"></figure>
<p>最后，triplet margin loss如下：</p>
<figure data-type="image" tabindex="19"><img src="https://jinyu-m.github.io/post-images/1604543289679.png" alt="" loading="lazy"></figure>
<p>通过该loss可以让descriptor的distinctiveness提升，而为了挑选去更具重复性的特征，D2在triple margin loss前加了一个权重</p>
<figure data-type="image" tabindex="20"><img src="https://jinyu-m.github.io/post-images/1604543298015.png" alt="" loading="lazy"></figure>
<p>这样的话，为了让loss降低，网络需要学习去提取区分度更高（m更小）并且可重复性更好（权重更大）的点，并且优化提取的descriptor。</p>
<h2 id="training-test"><em>Training &amp; Test</em></h2>
<p>D2采用了VGG-16网络模型（~conv4_3），加载imagenet预训练模型即可提取特征，如果进行finetune只需训练最后一层。在测试时，最后一个max-pooling改为average-pooling，并且stride改为1（不降低分辨率），conv4_1到conv4_3使用空洞卷积，这样得到的feature map是1/4大小的，D2使用SIFT中的local refinement方法去修正关键点位置，descriptor被双线性插值到矫正后的位置。</p>
<h2 id="evaluation"><em>Evaluation</em></h2>
<figure data-type="image" tabindex="21"><img src="https://jinyu-m.github.io/post-images/1604544099886.png" alt="" loading="lazy"></figure>
<p>在HPatches上，D2表现较差，threshold小于6px时的MMA很低。D2提取的特征较多，匹配数量也较多。在camera localization和3D reconstruction实验中效果较好。</p>
<h2 id="一些见解-2"><em>一些见解</em></h2>
<p>D2在strict matching上的表现较差，但是在localization和reconstruction任务上却表现不错。这其中的原因值得思考，我们一般做slam时要求特征更快更准，“更准”的要求真的必要么？</p>
<p>在inference的时候，网络的forward速度很快，~x ms，但是detection和恢复position误差的部分都需要~x00 ms。感觉可以从detection的方法上改进一下。</p>
<p>在我的试验中，发现D2其实提取的soft detection score还是很关注纹理复杂的区域的，但是这些区域很可能是dynamics，所以加入语义的方法进行筛选可能是个点。</p>
<p>D2定位精度差的原因，我觉得是因为它是在1/8的feature map上进行训练，在1/4的feature map上进行inference，所以直接插值到原分辨率造成了这种差异，因为在hpatches上threshold大于6px时D2的效果还是不错。</p>
<p>D2在训练时有考虑repeatable（因为直接用correspondence训练的）和disciminative，但是我觉得这种用triplets或者quadruplets去采样anchor、positive和negative会不会造成其实对disciminative的提升不大呢？</p>
<hr>
<h1 id="r2d2-pdf-code">R2D2 <a href="http://arxiv.org/abs/1906.06195">pdf</a> <a href="https://github.com/naver/r2d2">code</a></h1>
<h2 id="abstract-4"><em>Abstract</em></h2>
<p>作者在这篇工作中，提出一个观点：</p>
<blockquote>
<p>In this work, we argue that salient regions are not necessarily discriminative, and therefore an harm the performance of the description. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence.</p>
</blockquote>
<p>简单来说，就是之前的训练detector的方法，都是更关注检测出的特征是否repeative，这样的特征可能不够discriminative，比如重复性的纹理。所以作者认为，这样的训练方法有弊端。并且由于提取的salient region不够discriminative，所以descriptor的训练也不够完善。所以这篇工作中，作者要提取sparse，repeative and disciminative特征，依旧使用detect-and-descibe的提取方法。</p>
<h2 id="introduction-3"><em>Introduction</em></h2>
<figure data-type="image" tabindex="22"><img src="https://jinyu-m.github.io/post-images/1604543612648.png" alt="" loading="lazy"></figure>
<p>作者先更详细直观地阐述了一下motivation，就像上面这两幅示例图。第一幅图中，对于detector来说，只有黑三角形附近的区域是有利于提取keypoint的，但是所有包含该三角形的patch都可以提取同等可靠的descriptor，所以匹配的时候会有误差。第二幅图中，对于detector，所有棋盘网格的角点都可以提取出同等可靠的keypoint，但是它们都不利于提取descriptor，因为完全重复，匹配的时候会出现混淆。</p>
<p>所以，在这篇论文，作者认为detection和description是不可分割的，要一起训练（其实就是detect-and-descibe），并且提取的特征不关要repeatable还需要reliable for matching（感觉就是discrimative）。所以R2D2会分别根据basenet输出的feature map得到两张repeatability confidence map和reliability confidence map，然后综合两张map提取特征。</p>
<h2 id="methods"><em>Methods</em></h2>
<figure data-type="image" tabindex="23"><img src="https://jinyu-m.github.io/post-images/1604543624200.png" alt="" loading="lazy"></figure>
<p>R2D2采用全卷积网络结构，输入H x W大小的图像，输出三部分：第一部分D x H x W feature map X，视作H x W个D维局部特征，每个像素对应一个局部特征；第二部分是一个H x W heatmap S，每个位置上的值代表该点特征的sparse和repeatable，值在[0,1]之间，为了获得稀疏的特征，只提取S中局部最大值作为特征；第三部分是H x W heatmap R，对应特征的reliability或者说disciminativeness.</p>
<p>网络的backbone选用L2-net（不降低分辨率），但是将最后一个8x8的卷积换做3个2x2卷积，来减少参数。D=128。为了获得S和R，在backbone后接了两个1x1卷积+softmax。</p>
<h2 id="loss-3"><em>Loss</em></h2>
<h3 id="repeatability">repeatability</h3>
<p>首先训练特征的repeatability，作者认为</p>
<blockquote>
<p>In fact, using supervision essentially boils down in this case to copying an existing detector rather than discovering better and easier keypoints.</p>
</blockquote>
<p>就是用监督学习的训练方法，只是在学习那些现成的detector的检测策略（这个意义上讲，superpoint其实也是这样的，只不过通过homographic adaptation进行了一个data augmentation，去提升性能）。所以作者希望直接训练S，让其跟随图像变换而一起变换。</p>
<p>假设图1，图2，当图像是真实图像时，用optical flow或stereo matching的方法去获得两幅图像中像素级的对应关系U，当图像是虚拟的仿真图像时，那么U可以直接获得了。分别获得两幅图像的S1和S2，利用U将S2对应到S2U。如果S是covariant to transformations，那么S1和S2U应该是一致的。</p>
<p>所以，可以直接对S1和S2U求取cosine相似度，相似度越大，说明S的表现越好，但是warp后可能会出现occlusions、warp artifacts or border effects，所以作者用了一个局部的cosine相似度，求取多个patch的cosine相似度：</p>
<figure data-type="image" tabindex="24"><img src="https://jinyu-m.github.io/post-images/1604543638431.png" alt="" loading="lazy"></figure>
<p>Lcosine只能保证S1和S2U相似，但是容易导致S1和S2U变成常值。由于最后使用S是要挑选local maxima，所以还需要让S的局部峰值变大：</p>
<figure data-type="image" tabindex="25"><img src="https://jinyu-m.github.io/post-images/1604543644970.png" alt="" loading="lazy"></figure>
<p>最后，训练repeatability的loss就是以上两部分的加权：</p>
<figure data-type="image" tabindex="26"><img src="https://jinyu-m.github.io/post-images/1604543649872.png" alt="" loading="lazy"></figure>
<h3 id="reliability-ie-discriminativeness">Reliability, i.e., Discriminativeness</h3>
<p>这部分是为了让具有得到一个可以度量discriminative的score map，让具有discriminative descriptor的区域具有较大的可信度。</p>
<p>使用Average Precision Loss进行训练descriptor，就是给一对ground-truth batch，计算batch1中每个descriptor与batch2中每个des之间的距离，然后计算batch中每个query的AP loss**(?)**，用下面公式进行训练：</p>
<figure data-type="image" tabindex="27"><img src="https://jinyu-m.github.io/post-images/1604543659256.png" alt="" loading="lazy"></figure>
<p>这篇论文中也用到了AP loss，区别在于原本AP loss使用标准的keypoint detector去提取ground-truth batch，而在这里根据前文可以知道，提供了U，所以直接用U就可以获得ground-truth batch了。</p>
<p>在这一部分，作者还提出，想要提取利于匹配的特征，不光要考虑图像纹理的丰富度，还要考虑其是否discriminative。所以作者用R去筛选discriminative region中的特征，只有这部分特征会对网络训练产生影响。</p>
<figure data-type="image" tabindex="28"><img src="https://jinyu-m.github.io/post-images/1604543707448.png" alt="" loading="lazy"></figure>
<p>使用这种loss，为了使loss减小，当AP(i,j)小于k时，即该点descriptor不够discriminative，那么Rij应当为0,；当该点descriptor足够discriminative时，Rij应当为1。</p>
<h2 id="test"><em>Test</em></h2>
<p>在测试时，也采用了图像金字塔去获取更丰富的特征，从原分辨率开始，逐渐下采样，直到图像小于128px。每次从图像中利用S的局部最大值提取特征，保存。最后从所有保存的特征中，根据SR挑选top-K特征。</p>
<h2 id="training-2"><em>Training</em></h2>
<p>R2D2需要获得图像间的ground-truth correspondence。所以作者提出两种方法，一种就是常规的图2是由图1经过一种确定的transformation变换而来的；另一种，是作者自己提出一种pipeline，区别于之前用完全利用sfm获得dense correspondence（感觉在说D2...），作者先用sfm生成图像的3D点与6DoF位姿（sparse），对于sufficient overlap图像，用sfm提供的2D correspondence计算F matrix（作者发现这比直接用图像位姿去算要更可靠），然后用EpicFlow获得高质量的dense correspondence，作者在DeepMatching中加入epipolar constraint去增强方法，在EpicFlow的第一步获得了semi-dense correspondence。但是光流法可能无法应对有遮挡的情况，所以对于DeepMatching的输出，作者计算了一个connected consistent neighbors的图，只保留属于较大（至少有20个matches）connected component的matches，然后用一个thresholded kernel density estimator在验证后的matches中估计一个mask，作为optical flow可信度的度量。</p>
<h2 id="data-2"><em>Data</em></h2>
<p>Oxford，Paris，Aachen Day-Night.</p>
<h2 id="results"><em>Results</em></h2>
<figure data-type="image" tabindex="29"><img src="https://jinyu-m.github.io/post-images/1604543745556.png" alt="" loading="lazy"></figure>
<p>这张图很直观的表现出r2d2的优点，对于repeatability来说（第2行图），天空是可重复性很高的，但是对于特征来说，由于天空有大量重复纹理或无纹理，所以不利于区分，所以其上的特征reliability很低，r2d2综合考虑了这两点，所以提取的特征比较好。</p>
<figure data-type="image" tabindex="30"><img src="https://jinyu-m.github.io/post-images/1604544063401.png" alt="" loading="lazy"></figure>
<p>在hseq上效果也很好。</p>
<h2 id="一些见解-3"><em>一些见解</em></h2>
<p>r2d2很明确的定义了特征的两种特性，repeatability和reliability，其中reliability就是discriminativeness。</p>
<p>目前bag of others中据说效果很好的一种特征了，很显式的将特征的repeatable和disciminative纳入loss中，让网络去学习。</p>
<p>完全没有人工标记的特征点了，所以是网络自己去学习判断和提取特征——superpoint</p>
<p>在原分辨率的feature map上提取特征，定位准确度高了——D2</p>
<p>考虑到了local maxima可能并不全是利于匹配的特征，要考虑disciminative——D2+SuperPoint</p>
<hr>
<h1 id="sekd-pdf-code">SEKD <a href="http://arxiv.org/abs/2006.05077">pdf</a> <a href="https://github.com/aliyun/Self-Evolving-Keypoint-Demo">code</a></h1>
<h2 id="abstract-5"><em>Abstract</em></h2>
<p>论文提出，现存的一些特征提取方法（hand-crafted or learnt）都没有考虑到detector和descriptor之间的相互促进作用，所以导致效果或多或少不尽人意。所以这篇文章，其实是设计了一种自监督训练框架，强调repeatability和reliability，用完全无标注的自然图像去学习特征。</p>
<h2 id="introduction-4"><em>Introduction</em></h2>
<p>作者更加细化的分别定义了detector和descriptor的repeatability和reliability：</p>
<figure data-type="image" tabindex="31"><img src="https://jinyu-m.github.io/post-images/1604544171601.png" alt="" loading="lazy"></figure>
<p>总的来说就是两大特性，细分为四部分：（1）Repeatability：detector的repeatability体现在如果两幅图像描述了同一场景，那么在图1中“看到”的一个keypoint在图2中也应该可以看到；descriptor的repeatability体现在相同真实位置的关键点在不同图像中应该是invariant；（2）Reliability（其实可以理解为我们常说的disciminativeness）：detector的reliability体现在给定描述子的计算方法，一个detected keypoint应该可靠的区别于其他点，直白点说就是应该落在利于分辨的区域，避开重复性纹理区域；descriptor的reliability体现在给定了detection方法，计算出的描述子应该足够区分这些detected keypoints。</p>
<p>思考一下上面的这些特性，其实repeatability特性是detector和descriptor自己的inherent property，而reliability则体现了detector与descriptor之间的interactive property。</p>
<p>这篇论文别出心裁，不一起训练detector和descriptor（SuperPoint是先训detector后训descriptor，D2和R2D2是一起训），而是采用了一种iterative training strategy。利用上面说的inherent and interactive property，去直到训练。</p>
<p>简单的说，找出所有具有reliable descriptor的keypoints（descriptor repeatability），作为ground-truth去训练detector（detector reliability），用优化后的detector去检测keypoints（detector repeatability），基于这些keypoints训练descriptor（descriptor reliability）。重复这一过程，直到模型收敛。这就是self-evolving framework。整个训练过程不需要带有标注的数据。</p>
<p>（其实，我感觉和SuperPoint中提到的Iterative Homographic Adaptation有些相似，不过SEKD将这个流程变为一个end-to-end的训练过程，从零开始训练了）</p>
<h2 id="architecture-2"><em>Architecture</em></h2>
<figure data-type="image" tabindex="32"><img src="https://jinyu-m.github.io/post-images/1604544204468.png" alt="" loading="lazy"></figure>
<p>SEKD采用了类似于SuperPoint的结构，backbone由1个卷积和9个ResNet_v2模块，得到1/4大小的feature map。detector branch由2个deconv和1个softmax构成，输出2 x H x W的map P，代表keypoint probability，为了提升定位精度，有两个来自低层feature map的shortcut。descriptor branch由1个ResNet_v2模块和1个bi_linear上采样层构成，输出C-d描述子。</p>
<h2 id="self-evolving"><em>Self-Evolving !</em></h2>
<figure data-type="image" tabindex="33"><img src="https://jinyu-m.github.io/post-images/1604566729992.png" alt="" loading="lazy"></figure>
<p>在训练过程中，网络利用两方面的监督：（1）关键点的选取，有可靠descriptor的point被认为是keypoint；（2）不同图像间的keypoints correspondence，这个通过用一张图像，经过affine transformation获得匹配图像，因此correspondence也可以直接获得。</p>
<p>训练的流程基本分为四步：</p>
<p>1.用detector branch得到keypoint probability map P，利用NMS筛选keypoint；</p>
<p>2.在这些keypoint上，通过增强descriptor的repeatability和reliability来更新descriptor branch；</p>
<p>3.计算keypoint，具有reliable（repeatable and distinct） descriptor的point被作为keypoints;</p>
<p>4.在这些新的keypoint上，根据detector的repeatability和reliability来更新detector branch。</p>
<h3 id="1detect-keypoints-using-detector">1.Detect Keypoints using Detector</h3>
<figure data-type="image" tabindex="34"><img src="https://jinyu-m.github.io/post-images/1604566746753.png" alt="" loading="lazy"></figure>
<p>具有较高相应的点被视为可能的keypoint，经过NMS，每张图像可以获得1000个keypoint。但是由于不同图像条件下，可能没法取到一样的keypoint，所以作者采用了affine adaption方法，即对于原始图像进行random affine transformation和color jitter，获得新的图像后经过网络，获得不同图像条件下的P，最后映射回原图，取平均，得到最后的P.</p>
<h3 id="2update-keypoint-descriptor">2.Update Keypoint Descriptor</h3>
<p>根据上一节，获得了一张图像I中的keypoints Q，对I和Q进行random affine transformation和color jitter H，得到I<sup>和Q</sup>，并且可以获得特征的匹配关系&lt;Q,Q^&gt;，那么根据descriptor repeatability，匹配特征的des应该很靠近，根据descriptor reliability，不匹配的特征应该有很好的区分度。所以作者使用了triplet loss with hardest example mining去训练descriptor。</p>
<figure data-type="image" tabindex="35"><img src="https://jinyu-m.github.io/post-images/1604566756477.png" alt="" loading="lazy"></figure>
<p>除此之外，由于网络使用共享参数的backbone，所以为了保证detection的结果不会变化，作者还加入了一个损失函数：</p>
<figure data-type="image" tabindex="36"><img src="https://jinyu-m.github.io/post-images/1604566764397.png" alt="" loading="lazy"></figure>
<p>其中N’表示更新后的N。所以，用于更新descriptor的总loss为：</p>
<figure data-type="image" tabindex="37"><img src="https://jinyu-m.github.io/post-images/1604566770713.png" alt="" loading="lazy"></figure>
<h3 id="3compute-keypoints-via-descriptor">3.Compute Keypoints via Descriptor</h3>
<p>更新了descriptor后，下一步是要从descriptor map中提取keypoint。特征的reliability可从repeatability和distinctiveness两方面度量。对于repeatability：</p>
<figure data-type="image" tabindex="38"><img src="https://jinyu-m.github.io/post-images/1604566777144.png" alt="" loading="lazy"></figure>
<p>D越低，说明descriptor在相同位置上更靠近，repeatability更好。而对于distinctiveness：</p>
<figure data-type="image" tabindex="39"><img src="https://jinyu-m.github.io/post-images/1604566782560.png" alt="" loading="lazy"></figure>
<p>D‘越大，说明descriptor在不同位置的差异越大，distinctiveness更好。所以综合两点，特征的reliability定义为：</p>
<figure data-type="image" tabindex="40"><img src="https://jinyu-m.github.io/post-images/1604566791572.png" alt="" loading="lazy"></figure>
<p>R越大，说明特征更reliable。</p>
<p>由于匹配的图像对是用affine transformation获得的，所以图像中可能一些点没有对应的R，所以这里依然采用了之前所用的affine adaption方法来获得一张average R。</p>
<p>另外，计算D‘的计算量很大，所以作者在邻域内计算D'：</p>
<figure data-type="image" tabindex="41"><img src="https://jinyu-m.github.io/post-images/1604566800589.png" alt="" loading="lazy"></figure>
<p>在实验中，作者在1/4，1倍分辨率的descriptor map上分别计算R，然后将两个map融合，一起使用，以获得足够精细的R。</p>
<h3 id="4update-keypoint-detector">4.Update Keypoint detector</h3>
<p>根据descriptor计算出的keypoint可以看作ground-truth对detector进行训练，作者使用了focal loss：</p>
<figure data-type="image" tabindex="42"><img src="https://jinyu-m.github.io/post-images/1604566821435.png" alt="" loading="lazy"></figure>
<p>同时，为了减小匹配图像对上detection结果的差异，作者加入了：</p>
<figure data-type="image" tabindex="43"><img src="https://jinyu-m.github.io/post-images/1604566826991.png" alt="" loading="lazy"></figure>
<p>来训练detector的repeatability。并且还需要保证descriptor在这期间不受干扰，所以加入了：</p>
<figure data-type="image" tabindex="44"><img src="https://jinyu-m.github.io/post-images/1604566831492.png" alt="" loading="lazy"></figure>
<p>最后综合三部分，训练detector的loss为：</p>
<figure data-type="image" tabindex="45"><img src="https://jinyu-m.github.io/post-images/1604566836368.png" alt="" loading="lazy"></figure>
<h2 id="training-3"><em>Training</em></h2>
<p>特征在MS COCO验证集上完成训练。</p>
<p>训练进行了5个iteration，每个iteration中分别训练detector和descriptor20个epoches</p>
<h2 id="model-size"><em>Model Size</em></h2>
<figure data-type="image" tabindex="46"><img src="https://jinyu-m.github.io/post-images/1604566842806.png" alt="" loading="lazy"></figure>
<h2 id="performance"><em>Performance</em></h2>
<figure data-type="image" tabindex="47"><img src="https://jinyu-m.github.io/post-images/1604566848258.png" alt="" loading="lazy"></figure>
<hr>
<h1 id="l2-net-2017-pdf-code">L2-Net (2017) <a href="https://ieeexplore.ieee.org/document/8100132">pdf</a> <a href="https://github.com/yuruntian/L2-Net">code</a></h1>
<h2 id="abstract-6"><em>Abstract</em></h2>
<p>这篇论文提出了一种在欧拉空间中表现很好的CNN特征，突出点有四点：（1）提出了一种渐进的采样策略，使得网络可以在很少epoch内获得数以亿计的训练样本；（2）重视descriptor之间的相对距离；（3）对中间的feature map有格外的监督；（4）考虑des的compactness。</p>
<h2 id="architecture-3"><em>Architecture</em></h2>
<figure data-type="image" tabindex="48"><img src="https://jinyu-m.github.io/post-images/1604566899195.png" alt="" loading="lazy"></figure>
<p>网络结构如上图，在BN层中，作者设置权重为1，方差为0，不进行更新。在输出层，作者使用了local response normalization layer（LRN）来输出单位des。L2-Net将32 x 32的patch转变为128维的des。作者也使用了central-surround L2-Net，将两个L2-Net concat，左边的网络输入的是原始输入，右边的网络输入的是原本patch经过crop和resize后的中心部分（据说是可以处理scale不变性）。</p>
<h2 id="training-data"><em>Training data</em></h2>
<p>L2-Net用Hpatches和Brown数据集进行训练，这两个数据集都提供了matched pairs。</p>
<p>在加载训练数据时，从P个3D点中挑选p1个点，然后从P-p1中挑选p2个点，得到了p(p1+p2)个点，对每个p，随机获取一个匹配的patch，这样就获得了2p个训练数据作为网络的输入，记为X，X是32 x 32 x 2p维的。网路的输出记为Y，Y是128 x 2p维的。由于网络的输出经过了normalization，所以可以定义距离矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi><mo>=</mo><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo>(</mo><mn>2</mn><mi>x</mi><mo>(</mo><mn>1</mn><mo>−</mo><msubsup><mi>Y</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>Y</mi><mn>2</mn></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">D=sqrt(2 x (1 - Y_1^T Y_2))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4518920000000004em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>。</p>
<p>D中包含了p x p个pairs，对角线上的p个pair是positive matched pair，非对角线上的pair是negative pairs。<strong>(Q: 点都是随机取得，那在原本的p点中就可能会出现相互匹配的点，那么非对角线上的pair也不一定都是negative吧)</strong></p>
<h2 id="loss-function"><em>Loss function</em></h2>
<p>损失函数由三部分构成，第一部分，作者利用相对距离去约束匹配和非匹配的pair；第二部分，作者强调des的紧凑性，即des的各维信息之间应该没有相互关系；第三部分，作者对中间的feature map进行了约束。</p>
<h3 id="descriptor-similarity">descriptor similarity</h3>
<p>des之间的相互距离在pair是匹配的时候最小，所以体现在D中，就是对角线上的元素应当是行、列中最小的。定义行相似矩阵和列相似矩阵：</p>
<figure data-type="image" tabindex="49"><img src="https://jinyu-m.github.io/post-images/1604566957511.png" alt="" loading="lazy"></figure>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>S</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">S^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span></span></span></span></span></span></span></span>可以理解维<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>匹配到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的概率，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>S</mi><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">S^r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span></span></span>可以理解维<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>匹配到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的概率。为了让匹配的des之间距离减小，loss设计为：</p>
<figure data-type="image" tabindex="50"><img src="https://jinyu-m.github.io/post-images/1604566963223.png" alt="" loading="lazy"></figure>
<h3 id="descriptor-compactness">descriptor compactness</h3>
<p>作者发现过拟合问题是由于des各维度之间的correlation（我理解的是des出现了冗余）。所以作者加入了对des compactness的考虑。</p>
<p>作者设计了一个correlation matrix R：</p>
<figure data-type="image" tabindex="51"><img src="https://jinyu-m.github.io/post-images/1604566969526.png" alt="" loading="lazy"></figure>
<p>其中bi表示q个patch的des中第i维元素的集合，是个行向量。（我理解是对des的每一维调整均值后，计算cosine相似度，r=0，说明计算的两维des间正交，不相关）所以R的非对角线位置的元素需要靠近0。所以des compactness的loss为：</p>
<figure data-type="image" tabindex="52"><img src="https://jinyu-m.github.io/post-images/1604566974432.png" alt="" loading="lazy"></figure>
<p>如果加入了LRN后，每维数据的均值为0，所以Rs的计算可以简化为：</p>
<figure data-type="image" tabindex="53"><img src="https://jinyu-m.github.io/post-images/1604566979212.png" alt="" loading="lazy"></figure>
<h4 id="intermediate-feature-maps">Intermediate feature maps</h4>
<p>用E1的loss去计算网络中间的feature map上的similarity matrix G，然后构建了loss：</p>
<figure data-type="image" tabindex="54"><img src="https://jinyu-m.github.io/post-images/1604566984516.png" alt="" loading="lazy"></figure>
<p>作者称之为Discriminative Intermediate Feature maps (DIF)，并且发现在BN层后面加入DIF会提升效果，所以作者在第一个和最后一个BN层后计算了DIF。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#magicpoint-magic-leap-pdf">MagicPoint (Magic Leap) pdf</a>
<ul>
<li><a href="#abstract"><em>Abstract</em></a></li>
<li><a href="#introduction"><em>Introduction</em></a></li>
<li><a href="#method"><em>Method</em></a>
<ul>
<li><a href="#overview">overview</a></li>
<li><a href="#magicpoint">MagicPoint</a></li>
<li><a href="#magicwarp">MagicWarp</a></li>
</ul>
</li>
<li><a href="#%E4%B8%80%E4%BA%9B%E7%9C%8B%E6%B3%95"><em>一些看法</em></a></li>
</ul>
</li>
<li><a href="#superpoint-magic-leap-pdf-official-code-society-code">SuperPoint (Magic Leap) pdf official code society code</a>
<ul>
<li><a href="#abstract-2"><em>Abstract</em></a></li>
<li><a href="#introduction-2"><em>Introduction</em></a></li>
<li><a href="#architecture"><em>Architecture</em></a></li>
<li><a href="#loss"><em>Loss</em></a></li>
<li><a href="#training"><em>Training</em></a></li>
<li><a href="#homographic-adaptation"><em>Homographic Adaptation</em></a></li>
<li><a href="#%E4%B8%80%E4%BA%9B%E8%A7%81%E8%A7%A3"><em>一些见解</em></a></li>
</ul>
</li>
<li><a href="#d2-net-pdf-code">D2-net pdf code</a>
<ul>
<li><a href="#abstract-3"><em>Abstract</em></a></li>
<li><a href="#method-2"><em>Method</em></a></li>
<li><a href="#hard-feature-detectiontest"><em>Hard feature detection(test)</em></a></li>
<li><a href="#soft-feature-detectiontraining"><em>Soft feature detection(training)</em></a></li>
<li><a href="#multiscale-detection"><em>Multiscale detection</em></a></li>
<li><a href="#data"><em>Data</em></a></li>
<li><a href="#loss-2"><em>Loss</em></a></li>
<li><a href="#training-test"><em>Training &amp; Test</em></a></li>
<li><a href="#evaluation"><em>Evaluation</em></a></li>
<li><a href="#%E4%B8%80%E4%BA%9B%E8%A7%81%E8%A7%A3-2"><em>一些见解</em></a></li>
</ul>
</li>
<li><a href="#r2d2-pdf-code">R2D2 pdf code</a>
<ul>
<li><a href="#abstract-4"><em>Abstract</em></a></li>
<li><a href="#introduction-3"><em>Introduction</em></a></li>
<li><a href="#methods"><em>Methods</em></a></li>
<li><a href="#loss-3"><em>Loss</em></a>
<ul>
<li><a href="#repeatability">repeatability</a></li>
<li><a href="#reliability-ie-discriminativeness">Reliability, i.e., Discriminativeness</a></li>
</ul>
</li>
<li><a href="#test"><em>Test</em></a></li>
<li><a href="#training-2"><em>Training</em></a></li>
<li><a href="#data-2"><em>Data</em></a></li>
<li><a href="#results"><em>Results</em></a></li>
<li><a href="#%E4%B8%80%E4%BA%9B%E8%A7%81%E8%A7%A3-3"><em>一些见解</em></a></li>
</ul>
</li>
<li><a href="#sekd-pdf-code">SEKD pdf code</a>
<ul>
<li><a href="#abstract-5"><em>Abstract</em></a></li>
<li><a href="#introduction-4"><em>Introduction</em></a></li>
<li><a href="#architecture-2"><em>Architecture</em></a></li>
<li><a href="#self-evolving"><em>Self-Evolving !</em></a>
<ul>
<li><a href="#1detect-keypoints-using-detector">1.Detect Keypoints using Detector</a></li>
<li><a href="#2update-keypoint-descriptor">2.Update Keypoint Descriptor</a></li>
<li><a href="#3compute-keypoints-via-descriptor">3.Compute Keypoints via Descriptor</a></li>
<li><a href="#4update-keypoint-detector">4.Update Keypoint detector</a></li>
</ul>
</li>
<li><a href="#training-3"><em>Training</em></a></li>
<li><a href="#model-size"><em>Model Size</em></a></li>
<li><a href="#performance"><em>Performance</em></a></li>
</ul>
</li>
<li><a href="#l2-net-2017-pdf-code">L2-Net (2017) pdf code</a>
<ul>
<li><a href="#abstract-6"><em>Abstract</em></a></li>
<li><a href="#architecture-3"><em>Architecture</em></a></li>
<li><a href="#training-data"><em>Training data</em></a></li>
<li><a href="#loss-function"><em>Loss function</em></a>
<ul>
<li><a href="#descriptor-similarity">descriptor similarity</a></li>
<li><a href="#descriptor-compactness">descriptor compactness</a>
<ul>
<li><a href="#intermediate-feature-maps">Intermediate feature maps</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e0833a79ae313ee43ee5',
    clientSecret: 'b56f2b325b4ecd49f15ddec8d4795a0013debc77',
    repo: 'jinyu-m.github.io',
    owner: 'Jinyu-M',
    admin: ['Jinyu-M'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://jinyu-m.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
