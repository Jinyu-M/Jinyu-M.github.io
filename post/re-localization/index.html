<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Re-Localization | 年少万兜鍪</title>
<link rel="shortcut icon" href="https://jinyu-m.github.io/favicon.ico?v=1622028602248">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://jinyu-m.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Re-Localization | 年少万兜鍪 - Atom Feed" href="https://jinyu-m.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="写在前面
相比回环检测和场景识别，重定位任务不光需要检测出“位置”，还需要计算出“姿态”，这就需要2D-3D的关联，并且重定位问题也面临着季节、光照、动态遮挡等外观变化和视角变化等因素的影响。

目录
论文总结
*Posenet: A Co..." />
    <meta name="keywords" content="Re-localization" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://jinyu-m.github.io">
  <img class="avatar" src="https://jinyu-m.github.io/images/avatar.png?v=1622028602248" alt="">
  </a>
  <h1 class="site-title">
    年少万兜鍪
  </h1>
  <p class="site-description">
    少年吔
尘世恰好，有诗有酒刚好吐槽
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Re-Localization
            </h2>
            <div class="post-info">
              <span>
                2021-03-27
              </span>
              <span>
                14 min read
              </span>
              
                <a href="https://jinyu-m.github.io/tag/C5dppQYtP/" class="post-tag">
                  # Re-localization
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="写在前面">写在前面</h1>
<p>相比回环检测和场景识别，重定位任务不光需要检测出“位置”，还需要计算出“姿态”，这就需要2D-3D的关联，并且重定位问题也面临着季节、光照、动态遮挡等外观变化和视角变化等因素的影响。</p>
<!-- more -->
<h1 id="目录">目录</h1>
<p><a href="#0">论文总结</a><br>
<a href="#1">*Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015)</a><br>
<a href="#2">*Image-based Localization using Hourglass Networks (ICCVW 2017)</a><br>
<a href="#3">*Geometric loss functions for camera pose regression with deep learning (CVPR 2017)</a></p>
<hr>
<p><span id=0></span></p>
<h1 id="论文总结">论文总结</h1>
<table>
<thead>
<tr>
<th>method</th>
<th>resource</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#1">PoseNet</a></td>
<td>ICCV 2015</td>
<td>用一个神经网络直接预测输入图像的6DOF pose(3维位置+4维四元数表示的方向)。欧氏距离直接拟合，用SfM获得真值pose</td>
</tr>
<tr>
<td>CaTiLoc</td>
<td>ICASSP 2021</td>
<td>用MobileNet提取特征图，输入ViT，输出接全连接层，估计3D位置和4D四元数信息</td>
</tr>
<tr>
<td><a href="#2">Hourglass</a></td>
<td>ICCVW 2017</td>
<td>采用一个沙漏型的网络结构来尽可能保留图像的细节信息，从RGB图像直接预测图像的位姿</td>
</tr>
<tr>
<td><a href="#3">PoseNet2</a></td>
<td>CVPR 2017</td>
<td>在PoseNet的基础上，提出了两种新的损失函数，一种使用自动优化平衡参数的L1距离，一种使用重投影误差</td>
</tr>
</tbody>
</table>
<hr>
<p><span id=1></span></p>
<h1 id="posenet-a-convolutional-network-for-real-time-6-dof-camera-relocalizationiccv-2015-project">Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015) <a href="http://mi.eng.cam.ac.uk/projects/relocalisation/#dataset">project</a></h1>
<h2 id="abstract">Abstract</h2>
<p>作者在这篇论文中提出了一种鲁棒的、实时的单目6DOF重定位系统，该系统以end-to-end的方式训练了一个神经网络，来根据一幅RGB图像拟和6DOF相机位姿，无需额外的处理或图优化。该网络由23层卷积层构成，证明了卷积神经网络可以用来解决图像平面拟和问题。</p>
<h2 id="introduction">Introduction</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616982839161.png" alt="" loading="lazy"><br>
这篇论文中作者提出了一个可以拟合相机位姿的神经网络。为了实现这一目的，作者先利用从识别到定位的迁移学习，然后利用SfM从场景视频中自动产生训练数据（相机位姿），这样可以避免人工的标注。本文的另一贡献在于理解卷积神经网络产生的图像表征。作者展示了网络可以学到易于映射到位姿、且可以由一些额外的训练样本推广到看不到的场景的特征向量。<br>
基于视觉的重定位在粗略的相机定位任务中已经有了不错的表现，但是它们局限在优先的、离散的场景中，使得位姿估计成为一个分散、独立的系统。本文提出了一种直接从外观估计连续位姿的方法，而且场景可能包含多个目标，并且不需要被连续观察到。</p>
<h2 id="model-for-deep-regression-of-camera-pose">Model for deep regression of camera pose</h2>
<p>本文所设计的神经网络，输入是一张图像I，得到一个位姿向量p，包含相机的位置x和用四元数表示的朝向q：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mo>[</mo><mi>x</mi><mo separator="true">,</mo><mi>q</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">p=[x,q]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">]</span></span></span></span>，位姿是相对一个任意的全局参考系确定的。</p>
<h3 id="simultaneously-learning-location-and-orientation">Simultaneously learning location and orientation</h3>
<p>为了拟合位姿，作者使用如下损失函数：<br>
<img src="https://jinyu-m.github.io/post-images/1616984475751.png" alt="" loading="lazy"><br>
旋转的集合存在于四元数空间的单位球上。然而，欧氏距离损失函数无法使q保持在单位球上。作者发现，在训练过程中，q可以变得足够接近<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span>，以至于球面距离和欧式距离之间的区别变得不重要。为了简单且避免不必要的约束阻碍优化，我们选择忽略球面约束。<br>
<img src="https://jinyu-m.github.io/post-images/1616984689042.png" alt="" loading="lazy"><br>
我们发现，单独训练单个网络来回归位置和方向，与使用全6DOF姿态训练时相比，效果较差。如果只有位置或方向信息，卷积神经网络就不能有效地确定代表摄像机姿态的函数。我们也尝试将网络向下分支成两个独立的网络来回归位置和方向。然而，我们发现，由于相似的原因，它的有效性也较低。如果分开训练，那么位置和方向之间的相互影响的信息就会被忽略，进而影响训练。</p>
<h3 id="architecture">Architecture</h3>
<p>在实验中，作者使用了GoogLeNet作为基础来构建位姿回归网络。具体的调整如下：</p>
<ol>
<li>用affine regressors代替三个softmax分类器（GoogLeNet有两个辅助分类器）。softmax层被删除，每个全连接最后输出7维向量，表示3维的位置和4维的方向；</li>
<li>在最后的输出层前插入一个2048维的全连接层，用以生成一个可以探索泛化能力的局部特征特征向量；</li>
<li>在测试时，标准化四元数表示的方向向量，使其范数为1.<br>
对于输入图像，先将最小边调整为256，然后crop为224x224，输入网络。网络在训练时采用random crops（不影响相机位姿）在测试时，分别对中心裁剪后的图像和128个等间隔裁剪的图像进行位姿估计，最后平均其计算结构，作为输出。<br>
为了训练和测试，我们尝试在裁剪之前将原始图像缩放到不同的大小。放大输入相当于在下采样到一边长度为256之前裁剪输入。这增加了输入像素的空间分辨率。我们发现，这并没有提高定位性能，这表明对于重定位来说，context和视野比分辨率更重要。</li>
</ol>
<h2 id="dataset">Dataset</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616986113078.png" alt="" loading="lazy"><br>
作者提出了一个室外街景重定位数据集<a href="mi.eng.cam.ac.uk/projects/relocalisation/">Cambridge Landmarks</a>，该数据集包含5个场景。<br>
<img src="https://jinyu-m.github.io/post-images/1616986215367.png" alt="" loading="lazy"><br>
作者使用了7 scenes数据集作为室内场景的测试集。</p>
<h2 id="experiments">Experiments</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616986332346.png" alt="" loading="lazy"><br>
作者先和直接利用特征向量找训练集中最近邻的方法进行比较，说明神经网络可以更细致的回归位姿，具有超出训练集样本范围的泛化能力。<br>
<img src="https://jinyu-m.github.io/post-images/1616986570103.png" alt="" loading="lazy"><br>
还与RGB-D SCoRe Forest算法进行了比较。</p>
<hr>
<p><span id=2></span></p>
<h1 id="image-based-localization-using-hourglass-networks-pdf">Image-based Localization using Hourglass Networks <a href="https://ieeexplore.ieee.org/document/8265316">pdf</a></h1>
<h2 id="abstract-2">Abstract</h2>
<p>这篇论文提出一个根据单张RGB图像预测相机位姿的网络，网络为沙漏型，由一系列卷积和上卷积层和一个回归层组成。上卷积层用于保留图像的细节信息。</p>
<h2 id="introduction-2">Introduction</h2>
<p>受到视觉领域其他任务的启发，作者在通过RGB图像预测相机位姿的回归任务中，加入了更多上下文信息（细节信息），来采集图像中更全面的信息。该网络结构包含一个编码层，将所有信息编码压缩，和一个解码层，通过上卷积处理将编码层的特征图逐渐恢复到原图分辨率上。这样一个对称的编码层-解码层结构，称为沙漏型网络结构。本文的贡献在于：在CNN中加入了一系列上卷积层和shortcut连接，并将其用于相机定位任务中。</p>
<h2 id="method">Method</h2>
<p>网络的输入是RGB图像，网络的输出是七维的向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mo>[</mo><mi>q</mi><mo separator="true">,</mo><mi>t</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">p=[q,t]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">t</span><span class="mclose">]</span></span></span></span>，包含旋转四元数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi><mo>=</mo><mo>[</mo><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>4</mn></msub><mo>]</mo></mrow><annotation encoding="application/x-tex">q=[q_1,q_2,q_3,q_4]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>和平移向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>=</mo><mo>[</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>3</mn></msub><mo>]</mo></mrow><annotation encoding="application/x-tex">t=[t_1,t_2,t_3]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。<br>
<img src="https://jinyu-m.github.io/post-images/1622015490108.png" alt="" loading="lazy"><br>
网络结构的简图如图1所示，网络包括编码层、解码层和回归层。编码层是一个全卷积结构，用于提取特征。解码层包含多层上卷积层，来恢复细节信息。之后，用回归层来预测相机位姿。<br>
损失函数为：<br>
<img src="https://jinyu-m.github.io/post-images/1622015670265.png" alt="" loading="lazy"></p>
<h3 id="cnn-architecture">CNN Architecture</h3>
<p><img src="https://jinyu-m.github.io/post-images/1622015883330.png" alt="" loading="lazy"><br>
作者使用了ResNet34作为basenet，去掉最后的全连接层，去掉最后的softmax和average pooling层，作为网络的编码层。编码层的输出是分辨率为7x7的特征图。在编码层后，作者加入了三个上卷积层和一个卷积层，来保留必要的细节信息。最后接一个有三层全连接层的回归层。为了更好的保留细节信息，作者在编码层的四个残差模块和解码层对应的上卷积层与最后的卷积层之间加入了跳步连接。解码层的最后一个卷积层，不改变特征图的大小，只是用于减少通道数。网络详细结构如表1所示。<br>
<img src="https://jinyu-m.github.io/post-images/1622016297172.png" alt="" loading="lazy"></p>
<h3 id="evaluation-dataset">Evaluation Dataset</h3>
<p>7-Scenes数据集。</p>
<h2 id="experiments-2">Experiments</h2>
<p><img src="https://jinyu-m.github.io/post-images/1622016432427.png" alt="" loading="lazy"><br>
<img src="https://jinyu-m.github.io/post-images/1622018548319.png" alt="" loading="lazy"><br>
<img src="https://jinyu-m.github.io/post-images/1622018586115.png" alt="" loading="lazy"></p>
<hr>
<h1 id="geometric-loss-functions-for-camera-pose-regression-with-deep-learning-pdf">Geometric Loss Functions for Camera Pose Regression with Deep Learning <a href="https://ieeexplore.ieee.org/document/8100177">pdf</a></h1>
<h2 id="abstract-3">Abstract</h2>
<p>作者指出，PoseNet虽然可以利用高层特征进行定位，并且对光照、运动模糊和未知相机内参鲁棒，但是它的损失函数很简单，需要调试超参数。所以，这篇论文，作者探索了很多损失函数，基于几何和场景重投影误差来训练网络。</p>
<h2 id="model-for-camera-pose-regression">Model for Camera Pose Regression</h2>
<p>网络输入单目RGB图像，预测得到相机位姿p，包括三维位置向量x，和角度向量q，这里用四元数表示角度。位姿是相对任一全局参考帧确定的。在实践中，作者将全局参考帧设置为所有相机位姿的中心位置。</p>
<h3 id="architecture-2">Architecture</h3>
<p>作者用GooLeNet作为位姿回归网络，加载ImageNet预训练参数。并做出如下模型修改：1. 删掉最后用于分类的线性回归和softmax层；2. 增加全连接层，输出7维向量；3. 加入nomarlization层，将四元数归一化。</p>
<h3 id="pose-representation">Pose Representation</h3>
<p>相机的位置可以很容易的在欧式空间中表示。但是角度比较困难。作者比较了几种不同的方法，如欧拉角、角轴、SO(3)旋转矩阵和四元数。比较它们在训练中的功效。<br>
首先，欧拉角一般用于3D旋转的交互表示，会存在一些问题，比如它不是单射的（相差2<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>仍然表示用一个角），因此用于单模态标量回归任务。此外，它们不能为给定的角度提供唯一的参数化表示，会出现万向锁问题。角轴同样如此。<br>
旋转矩阵是旋转的过参数化表示，对于3D问题，旋转矩阵属于特殊正交群，必须满足标准正交性。在反向传播时，很难约束旋转矩阵的标准正交性。<br>
因此，作者选用四元数作为旋转的表示。任意单位长度的四维向量都可以映射到适当的旋转上，这比旋转角度更易于约束。它在单位球面上定义旋转，主要缺点是每个旋转都有两个映射，每个半球一个。</p>
<h3 id="loss-function">Loss Function</h3>
<h4 id="learning-position-and-orientation">Learning position and orientation</h4>
<p>可以通过一个欧式空间中的目标函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>x</mi></msub><mo>(</mo><mi>I</mi><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><mi>γ</mi></msub></mrow><annotation encoding="application/x-tex">L_x(I)={||x-\hat{x}||}_\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.185808em;vertical-align:-0.435808em;"></span><span class="mord"><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∣</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0016920000000000268em;"><span style="top:-2.4003000000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05556em;">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.435808em;"><span></span></span></span></span></span></span></span></span></span>来拟合相机位置。<br>
旋转可以用目标函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>q</mi></msub><mo>(</mo><mi>I</mi><mo>)</mo><mo>=</mo><msub><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo>−</mo><mfrac><mover accent="true"><mi>q</mi><mo>^</mo></mover><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><mi>γ</mi></msub></mrow><annotation encoding="application/x-tex">L_q(I)={||q-\frac{\hat{q}}{||\hat{q}||}||}_\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.638024em;vertical-align:-0.705808em;"></span><span class="mord"><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mtight">∣</span><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mtight">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mtight">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">∣</span><span class="mord">∣</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:-0.268308em;"><span style="top:-2.1303em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05556em;">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.705808em;"><span></span></span></span></span></span></span></span></span></span>。四元数的主要问题在于它不是单射的，每个旋转对应着两个值（分别位于一个半球上），即q与-q相同。为了解决这一问题，作者限制所有四元数位于一个半球上，让每个旋转只对应一个四元数。</p>
<h4 id="simultaneously-learning-position-and-orientation">Simultaneously learning position and orientation</h4>
<p>在PoseNet中，作者使用了一个简单的线性加和，来同时训练网络估计位置和角度：<br>
<img src="https://jinyu-m.github.io/post-images/1622024971103.png" alt="" loading="lazy"><br>
标量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>用于平衡两部分损失。作者发现只单独训位置或旋转效果不如联合训练好，说明单独训练会损失一些对于位置和旋转必要的信息。但是，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>需要很精细的调整。作者发现，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>在室外场景比较大（250-2000），在室内比较小（120-750）。作者用grid search去微调<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>.<br>
<img src="https://jinyu-m.github.io/post-images/1622025209171.png" alt="" loading="lazy"></p>
<h4 id="learning-an-optimal-weighting">Learning an optimal weighting</h4>
<p>作者旨在设计一个不需要超参数、来最优的学习估计位置和旋转。这篇论文中，作者提出了一种可以学习位置和旋转损失之间权重的损失函数。为此，作者使用homoscedastic uncertainty。homoscedastic uncertainty是一种不依赖于输入数据的不确定性度量，与heteroscedastic uncertainty相反。并且，它可以获取到任务本身的不确定性。<br>
<img src="https://jinyu-m.github.io/post-images/1622026009722.png" alt="" loading="lazy"><br>
其中，通过反向传播优化homoscedastic uncertainties <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>x</mi></msub><mn>2</mn></msup><mo separator="true">,</mo><msup><msub><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mi>q</mi></msub><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">{\hat{\sigma}_x}^2,{\hat{\sigma}_q}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.184556em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8984479999999999em;"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8984479999999999em;"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>。这些不确定性是自由的标量，表示homoscedastic（任务的）噪声。<br>
这个损失函数包含两个部分：拟合的参数和不确定性正则项。利用损失函数隐式地学习方差<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.方差越大，对回归的残差有调和作用；较大的方差(或不确定性)导致较小的残差损失。第二个正则项防止网络将不确定性估计为无穷大。<br>
在实践中，作者学习<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>s</mi><mo>^</mo></mover><mo>:</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><msup><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\hat{s}:=log {\hat{\sigma}}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0928879999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8984479999999999em;"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，因为它更稳定，可以避免分母为0，<br>
<img src="https://jinyu-m.github.io/post-images/1622026466459.png" alt="" loading="lazy"><br>
作者随机选择初值为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><msub><mi>s</mi><mi>x</mi></msub><mo>^</mo></mover><mo>=</mo><mn>0.0</mn><mo separator="true">,</mo><mover accent="true"><msub><mi>s</mi><mi>q</mi></msub><mo>^</mo></mover><mo>=</mo><mo>−</mo><mn>3.0</mn></mrow><annotation encoding="application/x-tex">\hat{s_x}=0.0,\hat{s_q}=-3.0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">3</span><span class="mord">.</span><span class="mord">0</span></span></span></span>.</p>
<h4 id="learning-from-geometric-reprojection-error">Learning from geometric reprojection error</h4>
<p>场景几何的重投影误差可以自然的将旋转和位移结合起来，得到一个损失标量。为了定义这个损失，作者首先定义了一个函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>将三维点g投影到二维图像坐标<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mo>(</mo><mi>u</mi><mo separator="true">,</mo><mi>v</mi><mo>)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">{(u,v)}^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.231231em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>上:<br>
<img src="https://jinyu-m.github.io/post-images/1622027437913.png" alt="" loading="lazy"><br>
其中，x和q表示相机位置和朝向，函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>定义为：<br>
<img src="https://jinyu-m.github.io/post-images/1622027489126.png" alt="" loading="lazy"><br>
其中K是相机内参矩阵，R是q对应的旋转矩阵。作者用根据真值位姿和估计位姿得到的重投影误差来定义损失，取图像I中可见的场景3D点几何<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>G</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">G&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>，损失函数为：<br>
<img src="https://jinyu-m.github.io/post-images/1622027650870.png" alt="" loading="lazy"><br>
由于该损失函数用真值和预测的位姿来投影3D点，所以可以使用任意的相机模型，只要相机内参相同即可，因此为了简单起见，坐这儿假设相机内参K为单位矩阵，不需要相机标定。</p>
<h4 id="regression-norm">Regression norm</h4>
<p>作者选用了表现最好的L1范数。</p>
<h2 id="experiments-3">Experiments</h2>
<p><img src="https://jinyu-m.github.io/post-images/1622028144600.png" alt="" loading="lazy"><br>
<img src="https://jinyu-m.github.io/post-images/1622028195909.png" alt="" loading="lazy"><br>
单独使用重投影误差损失函数从零开始训练，网络不收敛，但是用于fine-tune用优化参数的欧式距离损失函数训练的模型，会获得更好的表现。<br>
<img src="https://jinyu-m.github.io/post-images/1622028375175.png" alt="" loading="lazy"><br>
<img src="https://jinyu-m.github.io/post-images/1622028419990.png" alt="" loading="lazy"></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2">写在前面</a></li>
<li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="#%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93">论文总结</a></li>
<li><a href="#posenet-a-convolutional-network-for-real-time-6-dof-camera-relocalizationiccv-2015-project">Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015) project</a>
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-for-deep-regression-of-camera-pose">Model for deep regression of camera pose</a>
<ul>
<li><a href="#simultaneously-learning-location-and-orientation">Simultaneously learning location and orientation</a></li>
<li><a href="#architecture">Architecture</a></li>
</ul>
</li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#experiments">Experiments</a></li>
</ul>
</li>
<li><a href="#image-based-localization-using-hourglass-networks-pdf">Image-based Localization using Hourglass Networks pdf</a>
<ul>
<li><a href="#abstract-2">Abstract</a></li>
<li><a href="#introduction-2">Introduction</a></li>
<li><a href="#method">Method</a>
<ul>
<li><a href="#cnn-architecture">CNN Architecture</a></li>
<li><a href="#evaluation-dataset">Evaluation Dataset</a></li>
</ul>
</li>
<li><a href="#experiments-2">Experiments</a></li>
</ul>
</li>
<li><a href="#geometric-loss-functions-for-camera-pose-regression-with-deep-learning-pdf">Geometric Loss Functions for Camera Pose Regression with Deep Learning pdf</a>
<ul>
<li><a href="#abstract-3">Abstract</a></li>
<li><a href="#model-for-camera-pose-regression">Model for Camera Pose Regression</a>
<ul>
<li><a href="#architecture-2">Architecture</a></li>
<li><a href="#pose-representation">Pose Representation</a></li>
<li><a href="#loss-function">Loss Function</a>
<ul>
<li><a href="#learning-position-and-orientation">Learning position and orientation</a></li>
<li><a href="#simultaneously-learning-position-and-orientation">Simultaneously learning position and orientation</a></li>
<li><a href="#learning-an-optimal-weighting">Learning an optimal weighting</a></li>
<li><a href="#learning-from-geometric-reprojection-error">Learning from geometric reprojection error</a></li>
<li><a href="#regression-norm">Regression norm</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-3">Experiments</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://jinyu-m.github.io/post/place-recognition/">
              <h3 class="post-title">
                Place Recognition/Loop Closure Detection
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e0833a79ae313ee43ee5',
    clientSecret: 'b56f2b325b4ecd49f15ddec8d4795a0013debc77',
    repo: 'jinyu-m.github.io',
    owner: 'Jinyu-M',
    admin: ['Jinyu-M'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://jinyu-m.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
