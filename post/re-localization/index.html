<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Re-Localization | 年少万兜鍪</title>
<link rel="shortcut icon" href="https://jinyu-m.github.io/favicon.ico?v=1616988184658">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://jinyu-m.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Re-Localization | 年少万兜鍪 - Atom Feed" href="https://jinyu-m.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="写在前面
相比回环检测和场景识别，重定位任务不光需要检测出“位置”，还需要计算出“姿态”，这就需要2D-3D的关联，并且重定位问题也面临着季节、光照、动态遮挡等外观变化和视角变化等因素的影响。

目录
论文总结
*Posenet: A Co..." />
    <meta name="keywords" content="Re-localization" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://jinyu-m.github.io">
  <img class="avatar" src="https://jinyu-m.github.io/images/avatar.png?v=1616988184658" alt="">
  </a>
  <h1 class="site-title">
    年少万兜鍪
  </h1>
  <p class="site-description">
    少年吔
尘世恰好，有诗有酒刚好吐槽
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          文章
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Re-Localization
            </h2>
            <div class="post-info">
              <span>
                2021-03-27
              </span>
              <span>
                5 min read
              </span>
              
                <a href="https://jinyu-m.github.io/tag/C5dppQYtP/" class="post-tag">
                  # Re-localization
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="写在前面">写在前面</h1>
<p>相比回环检测和场景识别，重定位任务不光需要检测出“位置”，还需要计算出“姿态”，这就需要2D-3D的关联，并且重定位问题也面临着季节、光照、动态遮挡等外观变化和视角变化等因素的影响。</p>
<!-- more -->
<h1 id="目录">目录</h1>
<p><a href="#0">论文总结</a><br>
<a href="#1">*Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015)</a></p>
<hr>
<p><span id=0></span></p>
<h1 id="论文总结">论文总结</h1>
<table>
<thead>
<tr>
<th>method</th>
<th>resource</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PoseNet</td>
<td>ICCV2015</td>
<td>用一个神经网络直接预测输入图像的6DOF pose(3维位置+4维四元数表示的方向)。欧氏距离直接拟合，用SfM获得真值pose</td>
</tr>
</tbody>
</table>
<hr>
<p><span id=1></span></p>
<h1 id="posenet-a-convolutional-network-for-real-time-6-dof-camera-relocalizationiccv-2015-project">Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015) <a href="http://mi.eng.cam.ac.uk/projects/relocalisation/#dataset">project</a></h1>
<h2 id="abstract">Abstract</h2>
<p>作者在这篇论文中提出了一种鲁棒的、实时的单目6DOF重定位系统，该系统以end-to-end的方式训练了一个神经网络，来根据一幅RGB图像拟和6DOF相机位姿，无需额外的处理或图优化。该网络由23层卷积层构成，证明了卷积神经网络可以用来解决图像平面拟和问题。</p>
<h2 id="introduction">Introduction</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616982839161.png" alt="" loading="lazy"><br>
这篇论文中作者提出了一个可以拟合相机位姿的神经网络。为了实现这一目的，作者先利用从识别到定位的迁移学习，然后利用SfM从场景视频中自动产生训练数据（相机位姿），这样可以避免人工的标注。本文的另一贡献在于理解卷积神经网络产生的图像表征。作者展示了网络可以学到易于映射到位姿、且可以由一些额外的训练样本推广到看不到的场景的特征向量。<br>
基于视觉的重定位在粗略的相机定位任务中已经有了不错的表现，但是它们局限在优先的、离散的场景中，使得位姿估计成为一个分散、独立的系统。本文提出了一种直接从外观估计连续位姿的方法，而且场景可能包含多个目标，并且不需要被连续观察到。</p>
<h2 id="model-for-deep-regression-of-camera-pose">Model for deep regression of camera pose</h2>
<p>本文所设计的神经网络，输入是一张图像I，得到一个位姿向量p，包含相机的位置x和用四元数表示的朝向q：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mo>[</mo><mi>x</mi><mo separator="true">,</mo><mi>q</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">p=[x,q]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">]</span></span></span></span>，位姿是相对一个任意的全局参考系确定的。</p>
<h3 id="simultaneously-learning-location-and-orientation">Simultaneously learning location and orientation</h3>
<p>为了拟合位姿，作者使用如下损失函数：<br>
<img src="https://jinyu-m.github.io/post-images/1616984475751.png" alt="" loading="lazy"><br>
旋转的集合存在于四元数空间的单位球上。然而，欧氏距离损失函数无法使q保持在单位球上。作者发现，在训练过程中，q可以变得足够接近<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span>，以至于球面距离和欧式距离之间的区别变得不重要。为了简单且避免不必要的约束阻碍优化，我们选择忽略球面约束。<br>
<img src="https://jinyu-m.github.io/post-images/1616984689042.png" alt="" loading="lazy"><br>
我们发现，单独训练单个网络来回归位置和方向，与使用全6DOF姿态训练时相比，效果较差。如果只有位置或方向信息，卷积神经网络就不能有效地确定代表摄像机姿态的函数。我们也尝试将网络向下分支成两个独立的网络来回归位置和方向。然而，我们发现，由于相似的原因，它的有效性也较低。如果分开训练，那么位置和方向之间的相互影响的信息就会被忽略，进而影响训练。</p>
<h3 id="architecture">Architecture</h3>
<p>在实验中，作者使用了GoogLeNet作为基础来构建位姿回归网络。具体的调整如下：</p>
<ol>
<li>用affine regressors代替三个softmax分类器（GoogLeNet有两个辅助分类器）。softmax层被删除，每个全连接最后输出7维向量，表示3维的位置和4维的方向；</li>
<li>在最后的输出层前插入一个2048维的全连接层，用以生成一个可以探索泛化能力的局部特征特征向量；</li>
<li>在测试时，标准化四元数表示的方向向量，使其范数为1.<br>
对于输入图像，先将最小边调整为256，然后crop为224x224，输入网络。网络在训练时采用random crops（不影响相机位姿）在测试时，分别对中心裁剪后的图像和128个等间隔裁剪的图像进行位姿估计，最后平均其计算结构，作为输出。<br>
为了训练和测试，我们尝试在裁剪之前将原始图像缩放到不同的大小。放大输入相当于在下采样到一边长度为256之前裁剪输入。这增加了输入像素的空间分辨率。我们发现，这并没有提高定位性能，这表明对于重定位来说，context和视野比分辨率更重要。</li>
</ol>
<h2 id="dataset">Dataset</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616986113078.png" alt="" loading="lazy"><br>
作者提出了一个室外街景重定位数据集<a href="mi.eng.cam.ac.uk/projects/relocalisation/">Cambridge Landmarks</a>，该数据集包含5个场景。<br>
<img src="https://jinyu-m.github.io/post-images/1616986215367.png" alt="" loading="lazy"><br>
作者使用了7 scenes数据集作为室内场景的测试集。</p>
<h2 id="experiments">Experiments</h2>
<p><img src="https://jinyu-m.github.io/post-images/1616986332346.png" alt="" loading="lazy"><br>
作者先和直接利用特征向量找训练集中最近邻的方法进行比较，说明神经网络可以更细致的回归位姿，具有超出训练集样本范围的泛化能力。<br>
<img src="https://jinyu-m.github.io/post-images/1616986570103.png" alt="" loading="lazy"><br>
还与RGB-D SCoRe Forest算法进行了比较。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2">写在前面</a></li>
<li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="#%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93">论文总结</a></li>
<li><a href="#posenet-a-convolutional-network-for-real-time-6-dof-camera-relocalizationiccv-2015-project">Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization(ICCV 2015) project</a>
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-for-deep-regression-of-camera-pose">Model for deep regression of camera pose</a>
<ul>
<li><a href="#simultaneously-learning-location-and-orientation">Simultaneously learning location and orientation</a></li>
<li><a href="#architecture">Architecture</a></li>
</ul>
</li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#experiments">Experiments</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://jinyu-m.github.io/post/place-recognition/">
              <h3 class="post-title">
                Place Recognition/Loop Closure Detection
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e0833a79ae313ee43ee5',
    clientSecret: 'b56f2b325b4ecd49f15ddec8d4795a0013debc77',
    repo: 'jinyu-m.github.io',
    owner: 'Jinyu-M',
    admin: ['Jinyu-M'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://jinyu-m.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
